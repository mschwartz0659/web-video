{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ USPSA Shot Detection Model Training\n",
    "\n",
    "This notebook trains a machine learning model to detect gunshots in USPSA stage videos.\n",
    "\n",
    "## üìã Prerequisites:\n",
    "1. Upload your `training_data` folder to Google Drive\n",
    "2. Folder should contain:\n",
    "   - 20 video files (.mp4, .mov, etc.)\n",
    "   - 20 JSON label files\n",
    "\n",
    "## ‚ö° How to Run:\n",
    "1. Click **Runtime ‚Üí Run all** (or run cells one by one)\n",
    "2. When prompted, authorize Google Drive access\n",
    "3. Wait for training to complete (~30-60 minutes)\n",
    "4. Download the trained model at the end\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install -q librosa soundfile scikit-learn\n\n# Suppress warnings for cleaner output\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress TensorFlow warnings\n\nimport json\nimport numpy as np\nimport librosa\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nprint(\"‚úÖ Dependencies installed\")\nprint(f\"TensorFlow version: {tf.__version__}\")\nprint(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Step 2: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set your data path - UPDATE THIS if your folder name is different\n",
    "DATA_PATH = '/content/drive/MyDrive/uspsa_training_data'\n",
    "\n",
    "# Verify the folder exists\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    print(f\"‚ùå ERROR: Folder not found at {DATA_PATH}\")\n",
    "    print(\"Please update DATA_PATH to match your Google Drive folder\")\n",
    "else:\n",
    "    files = os.listdir(DATA_PATH)\n",
    "    videos = [f for f in files if f.endswith(('.mp4', '.mov', '.avi', '.webm', '.mkv'))]\n",
    "    jsons = [f for f in files if f.endswith('.json')]\n",
    "    print(f\"‚úÖ Found {len(videos)} videos and {len(jsons)} JSON files\")\n",
    "    if len(videos) != len(jsons):\n",
    "        print(\"‚ö†Ô∏è WARNING: Number of videos and JSON files don't match!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 3: Load and Validate Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def load_labels(data_path):\n    \"\"\"Load all JSON label files\"\"\"\n    labels_data = []\n    json_files = [f for f in os.listdir(data_path) if f.endswith('.json')]\n\n    for json_file in json_files:\n        with open(os.path.join(data_path, json_file), 'r') as f:\n            data = json.load(f)\n            labels_data.append(data)\n\n    return labels_data\n\n# Load all labels\nall_labels = load_labels(DATA_PATH)\n\n# Print summary\nprint(f\"üìã Loaded {len(all_labels)} label files\\n\")\n\ntotal_shots = 0\ntotal_beeps = 0\nfirst_person_count = 0\nthird_person_count = 0\n\nfor label_data in all_labels:\n    shots = sum(1 for l in label_data['labels'] if l['type'] == 'shot')\n    beeps = sum(1 for l in label_data['labels'] if l['type'] == 'beep')\n\n    total_shots += shots\n    total_beeps += beeps\n\n    if label_data['recording_type'] == 'first_person':\n        first_person_count += 1\n    else:\n        third_person_count += 1\n\n    print(f\"  {label_data['video_id']}: {shots} shots, {beeps} beeps ({label_data['recording_type']})\")\n\nprint(f\"\\nüìä Summary:\")\nprint(f\"  Total shots: {total_shots}\")\nprint(f\"  Total beeps: {total_beeps}\")\nprint(f\"  First-person videos: {first_person_count}\")\nprint(f\"  Third-person videos: {third_person_count}\")\nprint(f\"\\n  Total labeled examples: {total_shots + total_beeps}\")\n\n# Verify expected files\nprint(f\"\\n‚ö†Ô∏è VALIDATION:\")\nmissing_stages = []\nfor i in range(1, 11):\n    for suffix in ['a', 'b']:\n        expected_name = f\"stage{i}{suffix}\"\n        # Check if any label has this video_id (case-insensitive check)\n        found = any(label_data['video_id'].lower().startswith(expected_name.lower())\n                   for label_data in all_labels)\n        if not found:\n            missing_stages.append(expected_name)\n\nif missing_stages:\n    print(f\"  ‚ö†Ô∏è Missing expected videos: {', '.join(missing_stages)}\")\n    print(f\"  ‚Üí This is OK if you have different video names\")\nelse:\n    print(f\"  ‚úÖ All expected stage videos (stage1a-stage10b) found!\")\n\n# Check for unexpected files\nunexpected = []\nfor label_data in all_labels:\n    video_id = label_data['video_id'].lower()\n    if not any(f\"stage{i}\" in video_id for i in range(1, 99)):\n        unexpected.append(label_data['video_id'])\n\nif unexpected:\n    print(f\"  ‚ö†Ô∏è Unexpected video files (not stage format): {', '.join(unexpected)}\")\n    print(f\"  ‚Üí These will still be used for training\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéµ Step 4: Extract Audio Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def extract_audio_segment(video_path, timestamp, duration=0.15, sr=22050):\n    \"\"\"\n    Extract audio segment from video at specified timestamp\n    \n    Args:\n        video_path: Path to video file\n        timestamp: Time in seconds\n        duration: Duration of segment in seconds\n        sr: Sample rate\n    \n    Returns:\n        Mel spectrogram in dB scale with FIXED reference (preserves amplitude)\n    \"\"\"\n    try:\n        # Load audio from video\n        y, _ = librosa.load(video_path, sr=sr, offset=timestamp, duration=duration, mono=True)\n        \n        # Convert to mel spectrogram (visual representation of audio)\n        mel_spec = librosa.feature.melspectrogram(\n            y=y,\n            sr=sr,\n            n_mels=64,  # 64 frequency bands\n            fmax=8000,  # Focus on frequencies up to 8kHz (where gunshots are)\n            hop_length=512\n        )\n        \n        # **CRITICAL FIX: Use FIXED reference value instead of np.max**\n        # This preserves the absolute amplitude differences between loud shots and quiet background\n        mel_spec_db = librosa.power_to_db(mel_spec, ref=1.0)\n        \n        # Resize to fixed dimensions (64x64)\n        if mel_spec_db.shape[1] < 64:\n            # Pad if too short\n            mel_spec_db = np.pad(mel_spec_db, ((0, 0), (0, 64 - mel_spec_db.shape[1])), mode='constant')\n        else:\n            # Truncate if too long\n            mel_spec_db = mel_spec_db[:, :64]\n        \n        # Return without normalization - will be normalized globally later\n        return mel_spec_db\n    \n    except Exception as e:\n        print(f\"Error extracting audio: {e}\")\n        return None\n\nprint(\"‚úÖ Audio extraction function ready (with FIXED dB reference)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Step 5: Prepare Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def prepare_dataset(labels_data, data_path, non_shots_per_video=20):\n    \"\"\"\n    Extract features and labels from all videos for 3-CLASS CLASSIFICATION\n    \n    **AUTO-GENERATES non-shot samples** by randomly sampling timestamps\n    that are far from any labeled shots or beeps.\n\n    Args:\n        labels_data: List of label data from JSON files\n        data_path: Path to video directory\n        non_shots_per_video: Number of random non-shot samples to generate per video\n\n    Returns:\n        X: Array of spectrograms (features)\n        y: Array of labels (0 = non-shot, 1 = shot, 2 = beep)\n        metadata: List of dicts with info about each sample\n    \"\"\"\n    X = []\n    y = []\n    metadata = []\n\n    # Build case-insensitive file lookup\n    actual_files = {}\n    for filename in os.listdir(data_path):\n        if filename.endswith(('.mp4', '.mov', '.avi', '.webm', '.mkv', '.MP4', '.MOV', '.AVI', '.WEBM', '.MKV')):\n            actual_files[filename.lower()] = filename\n\n    print(f\"Found {len(actual_files)} video files in directory\\n\")\n\n    for label_file in labels_data:\n        video_id = label_file['video_id']\n\n        # Try case-insensitive match\n        video_filename = actual_files.get(video_id.lower())\n\n        if video_filename is None:\n            print(f\"‚ö†Ô∏è Video not found: {video_id}\")\n            print(f\"   (Checked for case-insensitive match)\")\n            continue\n\n        video_path = os.path.join(data_path, video_filename)\n\n        if not os.path.exists(video_path):\n            print(f\"‚ö†Ô∏è Video not found: {video_path}\")\n            continue\n\n        print(f\"Processing {video_filename}...\")\n\n        # Collect excluded times (shots and beeps)\n        excluded_times = []\n        \n        # Process labeled shots and beeps\n        for label in label_file['labels']:\n            if label['type'] == 'shot':\n                # Extract audio features for shot\n                spec = extract_audio_segment(video_path, label['time'])\n                if spec is not None:\n                    X.append(spec)\n                    y.append(1)  # 1 = shot\n                    metadata.append({\n                        'video_id': video_filename,\n                        'time': label['time'],\n                        'type': 'shot',\n                        'recording_type': label_file['recording_type']\n                    })\n                    excluded_times.append(label['time'])\n                    \n            elif label['type'] == 'beep':\n                # Extract audio features for beep\n                spec = extract_audio_segment(video_path, label['time'])\n                if spec is not None:\n                    X.append(spec)\n                    y.append(2)  # 2 = beep\n                    metadata.append({\n                        'video_id': video_filename,\n                        'time': label['time'],\n                        'type': 'beep',\n                        'recording_type': label_file['recording_type']\n                    })\n                    excluded_times.append(label['time'])\n\n        # Get video duration\n        try:\n            duration = librosa.get_duration(path=video_path)\n        except:\n            print(f\"  ‚ö†Ô∏è Could not get duration for {video_filename}, skipping non-shot generation\")\n            continue\n\n        # AUTO-GENERATE non-shot samples\n        # Sample random times that are at least 0.5s away from any shot or beep\n        non_shot_count = 0\n        attempts = 0\n        max_attempts = non_shots_per_video * 10  # Try up to 10x to find good samples\n\n        while non_shot_count < non_shots_per_video and attempts < max_attempts:\n            attempts += 1\n            \n            # Random timestamp (avoid first/last 1 second)\n            random_time = np.random.uniform(1.0, max(2.0, duration - 1.0))\n            \n            # Check if it's far enough from any shot/beep (at least 0.5s away)\n            min_distance = min([abs(random_time - t) for t in excluded_times]) if excluded_times else 999\n            \n            if min_distance >= 0.5:\n                # Extract audio features for non-shot\n                spec = extract_audio_segment(video_path, random_time)\n                if spec is not None:\n                    X.append(spec)\n                    y.append(0)  # 0 = non-shot\n                    metadata.append({\n                        'video_id': video_filename,\n                        'time': random_time,\n                        'type': 'non_shot_auto',\n                        'recording_type': label_file['recording_type']\n                    })\n                    non_shot_count += 1\n\n        print(f\"  Generated {non_shot_count} random non-shot samples\")\n\n    X = np.array(X)\n    y = np.array(y)\n\n    print(f\"\\n‚úÖ Dataset prepared:\")\n    print(f\"  Total samples: {len(X)}\")\n    print(f\"  Non-shots (class 0): {np.sum(y == 0)}\")\n    print(f\"  Shots (class 1): {np.sum(y == 1)}\")\n    print(f\"  Beeps (class 2): {np.sum(y == 2)}\")\n    print(f\"  Feature shape: {X.shape}\")\n\n    return X, y, metadata\n\n# Prepare the dataset with auto-generated non-shots\nX, y, metadata = prepare_dataset(all_labels, DATA_PATH, non_shots_per_video=20)"
  },
  {
   "cell_type": "markdown",
   "source": "## üîß Step 5.5: Apply Global Normalization (CRITICAL FIX)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print(\"\\n\" + \"=\"*60)\nprint(\"üîß APPLYING GLOBAL NORMALIZATION\")\nprint(\"=\"*60)\n\n# Calculate global min/max across ALL training data\nglobal_min = X.min()\nglobal_max = X.max()\n\nprint(f\"\\nGlobal statistics from {len(X)} samples:\")\nprint(f\"  Min: {global_min:.4f}\")\nprint(f\"  Max: {global_max:.4f}\")\nprint(f\"  Mean: {X.mean():.4f}\")\nprint(f\"  Std: {X.std():.4f}\")\n\n# Apply global normalization to all data\nX_normalized = (X - global_min) / (global_max - global_min + 1e-8)\n\nprint(f\"\\nAfter global normalization:\")\nprint(f\"  Min: {X_normalized.min():.4f}\")\nprint(f\"  Max: {X_normalized.max():.4f}\")\nprint(f\"  Mean: {X_normalized.mean():.4f}\")\nprint(f\"  Std: {X_normalized.std():.4f}\")\n\n# Replace X with normalized version\nX = X_normalized\n\n# **CRITICAL: Save normalization parameters for inference**\nnormalization_params = {\n    'global_min': float(global_min),\n    'global_max': float(global_max),\n    'sample_rate': 22050,\n    'segment_duration': 0.15,\n    'n_mels': 64,\n    'spec_size': 64\n}\n\n# Save to Google Drive\nnorm_params_path = '/content/drive/MyDrive/normalization_params.json'\nwith open(norm_params_path, 'w') as f:\n    json.dump(normalization_params, f, indent=2)\n\nprint(f\"\\n‚úÖ Normalization parameters saved to: {norm_params_path}\")\nprint(\"\\n‚ö†Ô∏è  IMPORTANT: Download this file along with the model!\")\nprint(\"   The server needs these values for inference!\")\n\nprint(\"=\"*60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì∏ Step 6: Visualize Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Show examples of non-shots, shots, and beeps\nfig, axes = plt.subplots(3, 4, figsize=(16, 12))\n\n# Find examples of each class\nnon_shot_indices = np.where(y == 0)[0][:4]\nshot_indices = np.where(y == 1)[0][:4]\nbeep_indices = np.where(y == 2)[0][:4] if np.sum(y == 2) > 0 else []\n\nfor i, idx in enumerate(non_shot_indices):\n    axes[0, i].imshow(X[idx], aspect='auto', origin='lower', cmap='viridis')\n    axes[0, i].set_title(f\"Non-shot: {metadata[idx]['video_id'][:20]}...\\n{metadata[idx]['time']:.2f}s\")\n    axes[0, i].axis('off')\n\nfor i, idx in enumerate(shot_indices):\n    axes[1, i].imshow(X[idx], aspect='auto', origin='lower', cmap='viridis')\n    axes[1, i].set_title(f\"Shot: {metadata[idx]['video_id'][:20]}...\\n{metadata[idx]['time']:.2f}s\")\n    axes[1, i].axis('off')\n\nif len(beep_indices) > 0:\n    for i, idx in enumerate(beep_indices):\n        axes[2, i].imshow(X[idx], aspect='auto', origin='lower', cmap='viridis')\n        axes[2, i].set_title(f\"Beep: {metadata[idx]['video_id'][:20]}...\\n{metadata[idx]['time']:.2f}s\")\n        axes[2, i].axis('off')\nelse:\n    for i in range(4):\n        axes[2, i].text(0.5, 0.5, 'No beep samples', ha='center', va='center')\n        axes[2, i].axis('off')\n\nplt.tight_layout()\nplt.suptitle('Spectrogram Examples: Non-Shots (top) vs Shots (middle) vs Beeps (bottom)', y=1.02, fontsize=14)\nplt.show()\n\nprint(\"üìä Notice the visual differences:\")\nprint(\"  - Non-shots: Random patterns or sustained noise\")\nprint(\"  - Shots: Bright vertical bands (sudden energy burst)\")\nprint(\"  - Beeps: Sustained tone with consistent frequency pattern\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÄ Step 7: Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Reshape for CNN (add channel dimension)\nX = X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)\n\n# Convert labels to categorical (one-hot encoding) for 3-class classification\nfrom tensorflow.keras.utils import to_categorical\ny_categorical = to_categorical(y, num_classes=3)\n\n# Split into train (70%), validation (15%), and test (15%)\nX_train, X_temp, y_train, y_temp = train_test_split(X, y_categorical, test_size=0.3, random_state=42, stratify=y)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp.argmax(axis=1))\n\n# Get class counts for each split\ny_train_classes = y_train.argmax(axis=1)\ny_val_classes = y_val.argmax(axis=1)\ny_test_classes = y_test.argmax(axis=1)\n\nprint(f\"‚úÖ Dataset split:\")\nprint(f\"  Training: {len(X_train)} samples ({len(X_train)/len(X)*100:.1f}%)\")\nprint(f\"  Validation: {len(X_val)} samples ({len(X_val)/len(X)*100:.1f}%)\")\nprint(f\"  Test: {len(X_test)} samples ({len(X_test)/len(X)*100:.1f}%)\")\nprint(f\"\\n  Train: {np.sum(y_train_classes == 0)} non-shots | {np.sum(y_train_classes == 1)} shots | {np.sum(y_train_classes == 2)} beeps\")\nprint(f\"  Val: {np.sum(y_val_classes == 0)} non-shots | {np.sum(y_val_classes == 1)} shots | {np.sum(y_val_classes == 2)} beeps\")\nprint(f\"  Test: {np.sum(y_test_classes == 0)} non-shots | {np.sum(y_test_classes == 1)} shots | {np.sum(y_test_classes == 2)} beeps\")\n\n# DIAGNOSTIC: Check if validation data looks normal\nprint(f\"\\nüîç VALIDATION SET DIAGNOSTICS:\")\nprint(f\"  Val data shape: {X_val.shape}\")\nprint(f\"  Val data min: {X_val.min():.4f}, max: {X_val.max():.4f}\")\nprint(f\"  Val data mean: {X_val.mean():.4f}, std: {X_val.std():.4f}\")\nprint(f\"  Any NaN values: {np.isnan(X_val).any()}\")\nprint(f\"  Any Inf values: {np.isinf(X_val).any()}\")\n\n# Check a few validation samples\nprint(f\"\\n  Sample validation features (first 5):\")\nfor i in range(min(5, len(X_val))):\n    class_id = y_val_classes[i]\n    class_name = ['non-shot', 'shot', 'beep'][class_id]\n    print(f\"    Val[{i}]: class={class_name}, min={X_val[i].min():.3f}, max={X_val[i].max():.3f}, mean={X_val[i].mean():.3f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Step 8: Build Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_model(input_shape=(64, 64, 1)):\n    \"\"\"\n    Create CNN model for 3-class shot detection\n    \n    Architecture:\n    - 3 Convolutional blocks (Conv2D + MaxPool + BatchNorm)\n    - Dense layers for classification  \n    - 3-class output (non-shot, shot, beep)\n    \"\"\"\n    model = keras.Sequential([\n        # Input layer\n        layers.Input(shape=input_shape),\n        \n        # Block 1\n        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n        layers.BatchNormalization(),\n        layers.MaxPooling2D((2, 2)),\n        \n        # Block 2\n        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n        layers.BatchNormalization(),\n        layers.MaxPooling2D((2, 2)),\n        \n        # Block 3\n        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n        layers.BatchNormalization(),\n        layers.MaxPooling2D((2, 2)),\n        \n        # Flatten and dense layers\n        layers.Flatten(),\n        layers.Dense(128, activation='relu'),\n        layers.Dense(64, activation='relu'),\n        \n        # Output layer (3-class classification: non-shot, shot, beep)\n        layers.Dense(3, activation='softmax')\n    ])\n    \n    # Compile model with categorical crossentropy for multi-class\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n        loss='categorical_crossentropy',\n        metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()]\n    )\n    \n    return model\n\n# Create the model\nmodel = create_model()\nmodel.summary()\n\nprint(f\"\\n‚úÖ Model created with {model.count_params():,} parameters\")\nprint(f\"üìä 3-class output: [non-shot, shot, beep]\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Step 9: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Calculate class weights to handle imbalance\nfrom sklearn.utils.class_weight import compute_class_weight\n\n# Get class labels from one-hot encoded y_train\ny_train_classes = y_train.argmax(axis=1)\n\n# Calculate automatic weights first\nclass_weights_array = compute_class_weight(\n    class_weight='balanced',\n    classes=np.unique(y_train_classes),\n    y=y_train_classes\n)\n\nprint(f\"‚öñÔ∏è Automatic class weights calculated:\")\nprint(f\"  Non-shot (0): {class_weights_array[0]:.2f}\")\nprint(f\"  Shot (1): {class_weights_array[1]:.2f}\")\nprint(f\"  Beep (2): {class_weights_array[2]:.2f}\")\n\n# **CRITICAL FIX: Cap beep weight to prevent overfitting**\n# Extreme beep weight causes model to memorize rather than learn\nclass_weights = {\n    0: class_weights_array[0],\n    1: class_weights_array[1],\n    2: min(class_weights_array[2], 5.0)  # Cap beep weight at 5.0\n}\n\nprint(f\"\\n‚úÖ Adjusted class weights (beep capped at 5.0 to prevent overfitting):\")\nprint(f\"  Non-shot (0): {class_weights[0]:.2f}\")\nprint(f\"  Shot (1): {class_weights[1]:.2f}\")\nprint(f\"  Beep (2): {class_weights[2]:.2f}\")\nprint(f\"  This balances learning without overfitting on rare classes\\n\")\n\n# Callbacks for training\ncallbacks = [\n    keras.callbacks.EarlyStopping(\n        monitor='val_loss',\n        patience=15,  # Increased patience to allow more learning\n        restore_best_weights=True,\n        verbose=1\n    ),\n    keras.callbacks.ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.5,\n        patience=7,  # Increased patience for learning rate reduction\n        min_lr=1e-7,\n        verbose=1\n    )\n]\n\n# Train the model with adjusted class weights\nprint(\"üöÄ Starting training...\\n\")\n\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    epochs=50,\n    batch_size=32,\n    class_weight=class_weights,  # ‚Üê Using adjusted weights\n    callbacks=callbacks,\n    verbose=1\n)\n\nprint(\"\\n‚úÖ Training complete!\")"
  },
  {
   "cell_type": "markdown",
   "source": "## üß™ Step 9.5: Verify Model Learned",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print(\"\\n\" + \"=\"*60)\nprint(\"üß™ VERIFYING MODEL LEARNED TO DISTINGUISH CLASSES\")\nprint(\"=\"*60)\n\n# Test on training samples\nnon_shot_indices = np.where(y_train_classes == 0)[0][:5]\nshot_indices = np.where(y_train_classes == 1)[0][:5]\nbeep_indices = np.where(y_train_classes == 2)[0][:5] if np.sum(y_train_classes == 2) > 0 else []\n\nclass_names = ['non-shot', 'shot', 'beep']\n\nprint(\"\\nPredictions on NON-SHOT samples (should predict class 0):\")\nfor idx in non_shot_indices:\n    pred = model.predict(X_train[idx:idx+1], verbose=0)[0]\n    predicted_class = pred.argmax()\n    confidence = pred[predicted_class] * 100\n    print(f\"  Non-shot: predicted={class_names[predicted_class]} ({confidence:.1f}%) {'‚úÖ' if predicted_class == 0 else '‚ùå FAILED'}\")\n\nprint(\"\\nPredictions on SHOT samples (should predict class 1):\")\nfor idx in shot_indices:\n    pred = model.predict(X_train[idx:idx+1], verbose=0)[0]\n    predicted_class = pred.argmax()\n    confidence = pred[predicted_class] * 100\n    print(f\"  Shot: predicted={class_names[predicted_class]} ({confidence:.1f}%) {'‚úÖ' if predicted_class == 1 else '‚ùå FAILED'}\")\n\nif len(beep_indices) > 0:\n    print(\"\\nPredictions on BEEP samples (should predict class 2):\")\n    for idx in beep_indices:\n        pred = model.predict(X_train[idx:idx+1], verbose=0)[0]\n        predicted_class = pred.argmax()\n        confidence = pred[predicted_class] * 100\n        print(f\"  Beep: predicted={class_names[predicted_class]} ({confidence:.1f}%) {'‚úÖ' if predicted_class == 2 else '‚ùå FAILED'}\")\n\n# Calculate average predictions per class\nprint(f\"\\nüìä Average confidence scores:\")\nfor class_id in range(3):\n    class_samples = X_train[y_train_classes == class_id][:20]\n    if len(class_samples) > 0:\n        preds = model.predict(class_samples, verbose=0)\n        avg_confidence = preds[:, class_id].mean() * 100\n        print(f\"  {class_names[class_id]}: {avg_confidence:.1f}% confident on own class\")\n\nprint(\"=\"*60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Step 10: Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot accuracy\n",
    "axes[0, 0].plot(history.history['accuracy'], label='Train')\n",
    "axes[0, 0].plot(history.history['val_accuracy'], label='Validation')\n",
    "axes[0, 0].set_title('Model Accuracy')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Plot loss\n",
    "axes[0, 1].plot(history.history['loss'], label='Train')\n",
    "axes[0, 1].plot(history.history['val_loss'], label='Validation')\n",
    "axes[0, 1].set_title('Model Loss')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Plot precision\n",
    "axes[1, 0].plot(history.history['precision'], label='Train')\n",
    "axes[1, 0].plot(history.history['val_precision'], label='Validation')\n",
    "axes[1, 0].set_title('Model Precision')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Precision')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Plot recall\n",
    "axes[1, 1].plot(history.history['recall'], label='Train')\n",
    "axes[1, 1].plot(history.history['val_recall'], label='Validation')\n",
    "axes[1, 1].set_title('Model Recall')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Recall')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Evaluate on test set\nprint(\"=\"*60)\nprint(\"üéØ EVALUATING ON TEST SET\")\nprint(\"=\"*60)\n\ntest_loss, test_acc, test_precision, test_recall = model.evaluate(X_test, y_test, verbose=0)\n\nprint(f\"\\nTest Set Performance:\")\nprint(f\"  Accuracy:  {test_acc*100:.2f}%\")\nprint(f\"  Precision: {test_precision*100:.2f}%\")\nprint(f\"  Recall:    {test_recall*100:.2f}%\")\nprint(f\"  Loss:      {test_loss:.4f}\")\n\n# Confusion matrix for 3 classes\ny_pred = model.predict(X_test, verbose=0).argmax(axis=1)\ny_true = y_test.argmax(axis=1)\ncm = confusion_matrix(y_true, y_pred)\n\nprint(f\"\\nConfusion Matrix:\")\nprint(f\"                 Predicted\")\nprint(f\"              Non-shot  Shot  Beep\")\nprint(f\"  Actual\")\nprint(f\"  Non-shot  [{cm[0][0]:5d}  {cm[0][1]:4d}  {cm[0][2]:4d}]\")\nprint(f\"  Shot      [{cm[1][0]:5d}  {cm[1][1]:4d}  {cm[1][2]:4d}]\")\nprint(f\"  Beep      [{cm[2][0]:5d}  {cm[2][1]:4d}  {cm[2][2]:4d}]\")\n\n# Per-class metrics\nprint(f\"\\nPer-Class Performance:\")\nfor class_id, class_name in enumerate(['Non-shot', 'Shot', 'Beep']):\n    class_mask = (y_true == class_id)\n    class_correct = np.sum((y_pred == class_id) & class_mask)\n    class_total = np.sum(class_mask)\n    if class_total > 0:\n        class_acc = class_correct / class_total * 100\n        print(f\"  {class_name}: {class_acc:.1f}% ({class_correct}/{class_total})\")\n\nprint(\"=\"*60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Save model (using modern Keras format)\nprint(\"=\"*60)\nprint(\"üíæ SAVING MODEL\")\nprint(\"=\"*60)\n\nlocal_model_path = '/content/shot_detector_model.keras'\nmodel.save(local_model_path)\nprint(f\"‚úÖ Model saved to: {local_model_path}\")\n\n# Also save to Google Drive  \ndrive_model_path = '/content/drive/MyDrive/shot_detector_model.keras'\nmodel.save(drive_model_path)\nprint(f\"‚úÖ Model also saved to Google Drive: {drive_model_path}\")\n\n# Save model metadata (including 3-class info)\nmodel_info = {\n    'model_type': '3-class',\n    'classes': ['non-shot', 'shot', 'beep'],\n    'test_accuracy': float(test_acc),\n    'test_precision': float(test_precision),\n    'test_recall': float(test_recall),\n    'total_training_samples': len(X_train),\n    'total_non_shots_trained': int(np.sum(y_train_classes == 0)),\n    'total_shots_trained': int(np.sum(y_train_classes == 1)),\n    'total_beeps_trained': int(np.sum(y_train_classes == 2)),\n    'input_shape': [64, 64, 1],\n    'sample_rate': 22050,\n    'segment_duration': 0.15\n}\n\nmetadata_path = '/content/drive/MyDrive/shot_detector_metadata.json'\nwith open(metadata_path, 'w') as f:\n    json.dump(model_info, f, indent=2)\nprint(f\"‚úÖ Metadata saved to: {metadata_path}\")\n\n# Download model\nprint(\"\\nüì• Downloading model...\")\nfrom google.colab import files\nfiles.download(local_model_path)\n\n# Download normalization parameters (CRITICAL!)\nprint(\"\\nüì• Downloading normalization parameters...\")\nfiles.download('/content/drive/MyDrive/normalization_params.json')\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"üéâ TRAINING COMPLETE!\")\nprint(\"=\"*50)\nprint(f\"\\nFinal Model Performance:\")\nprint(f\"  Test Accuracy: {test_acc*100:.2f}%\")\nprint(f\"  Test Precision: {test_precision*100:.2f}%\")\nprint(f\"  Test Recall: {test_recall*100:.2f}%\")\nprint(\"\\n‚úÖ Downloaded files:\")\nprint(\"  1. shot_detector_model.keras (3-class model)\")\nprint(\"  2. normalization_params.json (REQUIRED for server!)\")\nprint(\"\\nReplace both files in your project directory.\")\nprint(\"Server code will need to be updated for 3-class predictions.\")\nprint(\"=\"*50)",
   "metadata": {}
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}